{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "mYd2OavDtb-P",
    "outputId": "9fa17183-d53f-4d2b-d179-171085ae144f"
   },
   "source": [
    "Ali Baouan and Othmane Gaizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTxVyRTq2hWV"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import ta\n",
    "from ta.utils import *\n",
    "plt.style.use(\"Solarize_Light2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm6utSBRclHc"
   },
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbj2gOsoZMbW"
   },
   "source": [
    "Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xo79dhN3lNn"
   },
   "outputs": [],
   "source": [
    "#We label encode the galaxy name \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "train.galaxy=le.fit_transform(train.galaxy)\n",
    "test.galaxy=le.transform(test.galaxy)\n",
    "features=test.columns[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twAJyV7gZZoR"
   },
   "source": [
    "Removing some of the outlying rows ( for the log transformation of supposedly positive features  for example) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NK7Pm-nF-qf"
   },
   "outputs": [],
   "source": [
    "train = train[train['existence expectancy index'].notna()]\n",
    "train = train[train['existence expectancy at birth'].notna()]\n",
    "train = train[train['Gross income per capita']>0]\n",
    "train = train[train['Income Index'].notna()]\n",
    "train['Gross income per capita']=np.log(train['Gross income per capita'])\n",
    "test['Gross income per capita']=np.log(test['Gross income per capita'])\n",
    "train.loc[train['Estimated gross galactic income per capita, female']<0,'Estimated gross galactic income per capita, female']=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAUppVzH1HXO"
   },
   "outputs": [],
   "source": [
    "y_train=train['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQOprGFiZ-1n"
   },
   "source": [
    "We merge the train and test dataframes. We will use the total table to create new time-based features\n",
    "\n",
    "Then Retreive the X_train and X_test variables when needed using the galaxy+year indexation of the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPsiA93bZFGZ"
   },
   "outputs": [],
   "source": [
    "dataset=pd.concat((train[test.columns],test),axis=0)\n",
    "dataset=dataset.sort_values('galactic year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhQUMrvxGi0r"
   },
   "outputs": [],
   "source": [
    "#We define the features that might hold some sense when normalised annually\n",
    "features_annual_norm=['Mean years of education (galactic years)',\n",
    "'Intergalactic Development Index (IDI)',\n",
    "'Expected years of education (galactic years)',\n",
    "'Income Index',\n",
    "'Gross income per capita',\n",
    "'existence expectancy at birth',\n",
    "'existence expectancy index','Gross income per capita',\n",
    "'Estimated gross galactic income per capita, male',\n",
    "       'Estimated gross galactic income per capita, female','Education Index']\n",
    "def min_max(colum):\n",
    "    return (colum-colum.min())/(colum.max()-colum.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__8hRt1hmhO8"
   },
   "source": [
    "We normalize some features with respect to the annual mean\n",
    "\n",
    "We also calculate some country developement related features that might be relevant for the regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfcjpdFUaSgf"
   },
   "outputs": [],
   "source": [
    "#normalising features annualy\n",
    "dataset[features_annual_norm]\n",
    "\n",
    "for feat in features_annual_norm:\n",
    "    dataset[feat+\"_norm\"]=dataset[feat]/dataset.groupby('galactic year')[feat].transform('mean')\n",
    "features_to_ema=[feat+\"_norm\" for feat in features_annual_norm]\n",
    "features_to_ema=features_to_ema+['Life expectancy at birth, female (galactic years)','Life expectancy at birth, male (galactic years)','Life expectancy at birth, female (galactic years)','Life expectancy at birth, male (galactic years)']\n",
    "features_to_ema=features_to_ema+['Mean years of education (galactic years)','Expected years of education (galactic years)','Mean years of education, female (galactic years)','Mean years of education, male (galactic years)','Expected years of education, female (galactic years)','Expected years of education, male (galactic years)']\n",
    "#Construction of the life expectancy index as well as an correction constant to include gender inequalities. \n",
    "\n",
    "dataset['life_expectancy']=0.5*dataset['Life expectancy at birth, female (galactic years)']+0.5*dataset['Life expectancy at birth, male (galactic years)']\n",
    "dataset['Life_expectancy_index']=min_max(dataset['life_expectancy'])\n",
    "female_index=min_max(dataset['Life expectancy at birth, female (galactic years)'])\n",
    "male_index=min_max(dataset['Life expectancy at birth, male (galactic years)'])\n",
    "dataset['Life_expectancy_index_correction']=(2*np.sqrt(male_index*female_index))/(female_index+male_index)\n",
    "\n",
    "#Construction of the education_index as well as an correction constant to include gender inequalities. \n",
    "dataset['mean_year_education_index']=min_max(dataset['Mean years of education (galactic years)'])\n",
    "dataset['expec_year__education_index']=min_max(dataset['Expected years of education (galactic years)'])\n",
    "dataset['education_index']=min_max(np.sqrt(dataset['expec_year__education_index']*dataset['mean_year_education_index']))\n",
    "\n",
    "female_mean_index=min_max(dataset['Mean years of education, female (galactic years)'])\n",
    "male_mean_index=min_max(dataset['Mean years of education, male (galactic years)'])\n",
    "female_expec_index=min_max(dataset['Expected years of education, female (galactic years)'])\n",
    "male_expec_index=min_max(dataset['Expected years of education, male (galactic years)'])\n",
    "\n",
    "female_edu_index=min_max(np.sqrt(female_expec_index*female_mean_index))\n",
    "male_edu_index=min_max(np.sqrt(male_expec_index*male_mean_index))\n",
    "dataset['education_index_correction']=(2*np.sqrt(female_edu_index*male_edu_index))/(male_edu_index+male_edu_index)\n",
    "features_to_ema=features_to_ema+['existence expectancy at birth','Maternal mortality ratio (deaths per 100,000 live births)','Adolescent birth rate (births per 1,000 female creatures ages 15-19)','Population with at least some secondary education, female (% ages 25 and older)','Share of seats in senate (% held by female)','Labour force participation rate (% ages 15 and older), female','Population with at least some secondary education, male (% ages 25 and older)','Labour force participation rate (% ages 15 and older), male']\n",
    "dataset['income_index']=min_max(np.log(dataset['Gross income per capita'+\"_norm\"]))\n",
    "male_income_index=min_max(np.log(dataset['Estimated gross galactic income per capita, male'+\"_norm\"]))\n",
    "female_income_index=min_max(np.log(dataset['Estimated gross galactic income per capita, female'+\"_norm\"]))\n",
    "dataset['income_index_correction']=(2*np.sqrt(male_income_index*female_income_index))/(male_income_index+female_income_index)\n",
    "\n",
    "dataset['idh1'] = (min_max(dataset['existence expectancy at birth'])*dataset['education_index']*dataset['income_index'])**(1.0 / 3)\n",
    "dataset['idh']= (dataset['Life_expectancy_index']*dataset['education_index']*dataset['income_index'])**(1.0 / 3)\n",
    "dataset['iidh']=dataset['idh']*((dataset['Life_expectancy_index_correction']*dataset['education_index_correction']*dataset['income_index_correction'])**(1.0 / 3))\n",
    "dataset['G_female']=((((10/dataset['Maternal mortality ratio (deaths per 100,000 live births)'])*(1/dataset['Adolescent birth rate (births per 1,000 female creatures ages 15-19)']))**(0.5)) *((dataset['Population with at least some secondary education, female (% ages 25 and older)']*dataset['Share of seats in senate (% held by female)'])**(0.5)) *dataset['Labour force participation rate (% ages 15 and older), female']   )**(1./3)\n",
    "dataset['G_male']=(((dataset['Population with at least some secondary education, male (% ages 25 and older)']*(100.-dataset['Share of seats in senate (% held by female)']))**(0.5))*dataset['Labour force participation rate (% ages 15 and older), male'])**(1./3)\n",
    "\n",
    "dataset['Harm_G']=(2*np.sqrt(min_max(dataset['G_male'])*min_max(dataset['G_female'])))/(min_max(dataset['G_male'])+min_max(dataset['G_female']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5Oqd00FmvBV"
   },
   "source": [
    "We retreive the X_train dataframe from the merged dataset line by line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ISZIkXec4-e"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "  gal=train.galaxy.iloc[i]\n",
    "  year=train['galactic year'].iloc[i]\n",
    "  past_data=dataset[dataset['galactic year']==year]\n",
    "  wili=past_data[past_data.galaxy==gal]\n",
    "  \n",
    "  alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CwGXoUPhc47b",
    "outputId": "891d497c-19fb-4e9a-ef76-4edf0a3b4d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01924821 0.02267387 0.02231165 0.0204941  0.02279935]\n",
      "0.021505435979764346\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(colsample_bytree=0.3,\n",
    "                 gamma=0,       \n",
    "                 max_depth=4,\n",
    "                 min_child_weight=2.,\n",
    "                 n_estimators=222,                                                                    \n",
    "                 \n",
    "                 subsample=0.9,objective='reg:squarederror')\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=5)\n",
    "\n",
    "scores = -1*cross_val_score(model, X_train, y_train.iloc[0:], cv=cv, scoring=\"neg_root_mean_squared_error\")\n",
    "print(scores)\n",
    "print(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSa7FtUpvBIy"
   },
   "source": [
    "The performance is bad but we see some of the constructed features have some predictive ability. We will try to improve on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "F3u_LDOIgJTZ",
    "outputId": "23466ff1-05e0-4abf-8cc0-f4ea4d75aa38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idh1                                                                  0.113895\n",
       "income_index                                                          0.103804\n",
       "Intergalactic Development Index (IDI)_norm                            0.082407\n",
       "Renewable energy consumption (% of total final energy consumption)    0.048466\n",
       "Education Index_norm                                                  0.036912\n",
       "education_index                                                       0.031193\n",
       "Share of seats in senate (% held by female)                           0.027412\n",
       "Gross income per capita_norm                                          0.025887\n",
       "Intergalactic Development Index (IDI), Rank                           0.023353\n",
       "Intergalactic Development Index (IDI), male                           0.020926\n",
       "Gender Inequality Index (GII)                                         0.019648\n",
       "Education Index                                                       0.017568\n",
       "Interstellar Data Net users, total (% of population)                  0.016634\n",
       "G_female                                                              0.016279\n",
       "Expected years of education, female (galactic years)                  0.016275\n",
       "iidh                                                                  0.015183\n",
       "Intergalactic Development Index (IDI)                                 0.013819\n",
       "existence expectancy index_norm                                       0.013708\n",
       "Youth unemployment rate (female to male ratio)                        0.012422\n",
       "existence expectancy at birth                                         0.012030\n",
       "dtype: float32"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(colsample_bytree=0.3,\n",
    "                 gamma=0,       \n",
    "                 max_depth=4,\n",
    "                 min_child_weight=2.,\n",
    "                 n_estimators=222,                                                                    \n",
    "                 \n",
    "                 subsample=0.9,objective='reg:squarederror')\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "feature_imp = pd.Series(model.feature_importances_,index=list(X_train.columns)).sort_values(ascending=False)\n",
    "feature_imp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COd0CZEtvP4G"
   },
   "source": [
    "# Time-based Features:\n",
    "The problem can be **indexed** using only **galaxy** and year features: \n",
    "to every datapoint defined by galaxy and year and for every feature we will add the following features : \n",
    "\n",
    "**feature_cum_max**($gal,year$)=$max$(**feature**($gal,t$))$_{t<year}$\n",
    "\n",
    "**feature_ema**($gal,year$)=$ema$(**feature**($gal,t$))$_{t<year}$\n",
    "\n",
    "**future_feature_ema**($gal,year$)=$ema$(**feature**($gal,t$))$_{t=year+2,year+1,year}$\n",
    "\n",
    "\n",
    "We believe these features can tackle the problem of nan values in many features. \n",
    "\n",
    "The past based features are totally logical and can be set up in a realistic machine learning task. However in this task, we have access to the future feature values in the test set too ( we merge the datasets anyway) so we choose to incorporate future information as it can be predictive of the current trend. \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZVq_kv0m_Qt"
   },
   "source": [
    "We take the 80 most important features from the last regression + the constructed ones . The task is as follows \n",
    "\n",
    "We will construct the described time based features for all these \"important features\" and add them as columns later on .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrEEB-sIkMlQ"
   },
   "outputs": [],
   "source": [
    "temp_features=list(feature_imp.head(80).index)\n",
    "temp_features.remove('galaxy')\n",
    "features_to_ema=features_to_ema+temp_features\n",
    "features_to_ema=list(set(features_to_ema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "colab_type": "code",
    "id": "H1LwZkFVk4sw",
    "outputId": "725437a6-7b76-4338-c302-4a9bf3a5fc3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "galaxies=list(set(train.galaxy))\n",
    "dict={}\n",
    "for gal in galaxies:\n",
    "    dict[gal]=dataset[dataset.galaxy==gal]\n",
    "    for feat in features_to_ema:\n",
    "        a=ema(dict[gal][feat],2,True).fillna(0).copy()\n",
    "        dict[gal][feat+'_3']=a.copy()\n",
    "        a=ema(dict[gal][feat],6,True).fillna(0).copy()\n",
    "        dict[gal][feat+'_6']=a.copy() \n",
    "\n",
    "        dict[gal][feat+'_next']=dict[gal][feat].shift(-1)\n",
    "\n",
    "\n",
    "        a=(sma(dict[gal][feat+'_next'].iloc[::-1],3,True).fillna(0).iloc[::-1]).copy()\n",
    "        dict[gal][feat+'_fut_sma']=a.copy()\n",
    "        \n",
    "\n",
    "        a=dict[gal][feat+'_3'].cummin().copy()\n",
    "        dict[gal][feat+'_cumin']=a.copy()\n",
    "        a=dict[gal][feat+'_3'].cummax().copy() \n",
    "        dict[gal][feat+'_cumax']=a.copy()\n",
    "        \n",
    "        a=dict[gal][feat+'_3'].iloc[::-1].cummax().iloc[::-1].copy() \n",
    "        dict[gal][feat+'_fut_cumax']=a.copy()\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uha879kxlT0t"
   },
   "outputs": [],
   "source": [
    "dataset_years=dataset.copy()\n",
    "for feat in features_to_ema:\n",
    "    dataset_years[feat+\"_rank\"]=dataset.groupby('galactic year')[feat].rank(ascending=False)\n",
    "for feat in features_to_ema:\n",
    "    dataset_years[feat+\"_3\"]=np.full(len(dataset_years), np.nan)   \n",
    "    dataset_years[feat+'_6']=np.full(len(dataset_years), np.nan)\n",
    "    dataset_years[feat+'_next']=np.full(len(dataset_years), np.nan)\n",
    "    dataset_years[feat+'_fut_sma']=np.full(len(dataset_years), np.nan)  \n",
    "    dataset_years[feat+\"_cumin\"]=np.full(len(dataset_years), np.nan)   \n",
    "    dataset_years[feat+\"_cumax\"]=np.full(len(dataset_years), np.nan)\n",
    "    dataset_years[feat+\"_fut_cumax\"]=np.full(len(dataset_years), np.nan)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IN8xMHSyIf2"
   },
   "source": [
    "We now fill our Train-test dataset with the new features stored in each galaxy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cd4Q4G6flT5o"
   },
   "outputs": [],
   "source": [
    "for gal in galaxies:\n",
    "    for feat in features_to_ema:\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+\"_3\"]=dict[gal][feat+\"_3\"]\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+\"_6\"]=dict[gal][feat+\"_6\"]\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+'_next']=dict[gal][feat+'_next']\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+'_fut_sma']=dict[gal][feat+'_fut_sma']\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+\"_cumin\"]=dict[gal][feat+\"_cumin\"]\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+\"_cumax\"]=dict[gal][feat+\"_cumax\"]\n",
    "        dataset_years.loc[dataset_years.galaxy==gal,feat+\"_fut_cumax\"]=dict[gal][feat+\"_fut_cumax\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RgkMwFkn-Vg"
   },
   "source": [
    "We retreive  X_train from the merged dataset using the galaxy and year indexation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlkuwZg7nUN-"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "\n",
    "for i in range(len(train)):\n",
    "  gal=train.galaxy.iloc[i]\n",
    "  year=train['galactic year'].iloc[i]\n",
    "  past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "  wili=past_data[past_data.galaxy==gal]\n",
    "  \n",
    "  alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kxR22i9oQtV"
   },
   "source": [
    "These features divide by 2 the CV score. It is not the ideal metric for performance but it is considerable improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_1KAQn4Dowb0",
    "outputId": "156a8479-5872-4cc6-893c-fcb4d6a6c0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00400873 0.01037095 0.01005489 0.01025978 0.01013002]\n",
      "0.008964875635909683\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                eta=0.01,\n",
    "                 gamma=0,       \n",
    "                 max_depth=5,\n",
    "                 min_child_weight=2.,\n",
    "                 n_estimators=500,                                                                    \n",
    "                 \n",
    "                 subsample=1.,objective='reg:squarederror')\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=5)\n",
    "\n",
    "scores = -1*cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\")\n",
    "print(scores)\n",
    "print(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "dbOQmK7I2zCF",
    "outputId": "9631d6ed-4dfe-415e-dcea-679e394b7e5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idh1_cumin                                                                                    0.277964\n",
       "iidh_fut_cumax                                                                                0.168649\n",
       "idh1_fut_cumax                                                                                0.098416\n",
       "Intergalactic Development Index (IDI), Rank_fut_cumax                                         0.043727\n",
       "Life_expectancy_index_cumin                                                                   0.037431\n",
       "Life_expectancy_index_fut_cumax                                                               0.035197\n",
       "existence expectancy at birth_fut_cumax                                                       0.024907\n",
       "Share of seats in senate (% held by female)_cumax                                             0.024410\n",
       "Life expectancy at birth, female (galactic years)_cumin                                       0.015960\n",
       "Renewable energy consumption (% of total final energy consumption)_3                          0.014744\n",
       "Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))_fut_cumax    0.014636\n",
       "iidh_next                                                                                     0.013513\n",
       "Renewable energy consumption (% of total final energy consumption)_6                          0.012713\n",
       "G_female_cumax                                                                                0.011095\n",
       "Gender Inequality Index (GII)_fut_cumax                                                       0.011004\n",
       "Life expectancy at birth, male (galactic years)_cumax                                         0.009616\n",
       "Interstellar Data Net users, total (% of population)_fut_cumax                                0.009434\n",
       "Share of seats in senate (% held by female)_fut_cumax                                         0.008447\n",
       "Estimated gross galactic income per capita, female_norm_cumax                                 0.007511\n",
       "Estimated gross galactic income per capita, male_norm_cumax                                   0.007371\n",
       "dtype: float32"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "feature_imp = pd.Series(model.feature_importances_,index=list(X_train.columns)).sort_values(ascending=False)\n",
    "feature_imp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGBm3cOIokMv"
   },
   "source": [
    "# Target related features\n",
    "We noticed simple regression on the time for every galaxy gave an actual good performance. There was definitely an implicit low term trend present. We tried to leverage the eventual temporal dynamics of the target. \n",
    "\n",
    "Features of  **past targets** for a galaxy at a given year are logically available and could be used to help predict the current target. \n",
    "\n",
    "the feature we added is \n",
    "\n",
    "**last_y**($gal,year$)=**y**($gal,year-1$)\n",
    "\n",
    "The major problem of this feature is that, as can be seen in the following figure: the test points exist in time intervals, and therefore **last_y is not available for most points in the test set**. \n",
    "\n",
    "Even if it is a good predictor it might just ruin the test performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "ihFz8YEA1KWe",
    "outputId": "353eda27-a7a9-41ca-f905-0e80b99d8e02"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for gal in galaxies[1:3]:\n",
    " plt.plot(train[train.galaxy == gal]['galactic year'], np.ones(len(train[train.galaxy == gal]['galactic year'])), 'bx')\n",
    " plt.plot(test[test.galaxy == gal]['galactic year'], np.ones(len(test[test.galaxy == gal]['galactic year'])), 'rx')\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjF1GfhU1-wa"
   },
   "source": [
    "## We start by replacing the missing values by the last observed target for the galaxy. \n",
    "\n",
    "**We also added financial time series indicators, hoping they could give an idea of the trend of the target and its relation with the trends of the features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AENJ7lBr93g"
   },
   "outputs": [],
   "source": [
    "test_bis=test.copy()\n",
    "test_bis['y']=np.full(len(test),np.nan)\n",
    "\n",
    "dataset_bis=pd.concat((train,test_bis),axis=0)\n",
    "dataset_bis=dataset_bis.sort_values('galactic year')\n",
    "dataset_bis=dataset_bis[['galaxy','galactic year','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iCAPujkDr-BB",
    "outputId": "e972752e-1ac4-49ca-ee39-304ebe9aff35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_y={}\n",
    "for gal in galaxies:\n",
    "    dict_y[gal]=dataset_bis[dataset_bis.galaxy==gal]\n",
    "    dict_y[gal]['last_y']=(dict_y[gal]['y'].shift()).fillna(method='ffill')\n",
    "    dict_y[gal]['next_y']=(dict_y[gal]['y'].shift(-1)).fillna(method='bfill')\n",
    "    dict_y[gal]['next_y_2']=(dict_y[gal]['next_y'].shift(-1))\n",
    "    dict_y[gal]['next_y_3']=(dict_y[gal]['next_y_2'].shift(-1))\n",
    "    dict_y[gal]['last_likely']=np.log(dict_y[gal]['y'].shift()+0.01)\n",
    "\n",
    "    diff = dict_y[gal]['last_y'].diff(1)\n",
    "    up, down = diff.copy(), diff.copy()\n",
    "    diff[up < 0] = 0\n",
    "    diff[up > 0] = 1\n",
    "    dict_y[gal]['last_y_psy'] = sma(diff,4)\n",
    "    bb = ta.volatility.BollingerBands(dict_y[gal]['last_y'],n=6)\n",
    "    dict_y[gal]['last_y_bb_%b'] = bb.bollinger_pband()\n",
    "    dict_y[gal]['last_y_bb_bw'] = bb.bollinger_wband()\n",
    "    dict_y[gal]['last_y_MACD'] = ta.trend.macd(dict_y[gal]['last_y'], n_slow=5, n_fast=2)\n",
    "    Zscore=(dict_y[gal]['last_y']-sma(dict_y[gal]['last_y'],5))/dict_y[gal]['last_y'].rolling(5).std()\n",
    "    dict_y[gal]['last_y_Zscore']=Zscore\n",
    "    dict_y[gal]['last_likely_ema_4']=ema(dict_y[gal]['last_likely'],4,True).fillna(0)\n",
    "    dict_y[gal]['last_likely_ema_6']=ema(dict_y[gal]['last_likely'],6,True).fillna(0)\n",
    "    dict_y[gal]['last_y_ema_4']=ema(dict_y[gal]['last_y'],4,True).fillna(0)\n",
    "    dict_y[gal]['last_y_ema_6']=ema(dict_y[gal]['last_y'],6,True).fillna(0)\n",
    "    dict_y[gal]['last_y_ema_3']=ema(dict_y[gal]['last_y'],3,True).fillna(0)\n",
    "    dict_y[gal]['time_diff']=dict_y[gal]['galactic year'].diff()\n",
    "    dict_y[gal]['last_y_max']=dict_y[gal]['last_y'].rolling(5).max()\n",
    "    dict_y[gal]['last_y_min']=dict_y[gal]['last_y'].rolling(5).min()\n",
    "    #William %R\n",
    "    wri = ta.momentum.WilliamsRIndicator(high=dict_y[gal]['last_y_max'],low=dict_y[gal]['last_y_min'],close=dict_y[gal]['last_y'],lbp=6)\n",
    "    dict_y[gal]['last_y_WilliamsR'] = wri.wr()\n",
    "    #Relative momentum index\n",
    "    diff = dict_y[gal]['last_y'].diff(1)\n",
    "    up = diff.where(diff > 0, 0.0)\n",
    "    dn = -diff.where(diff < 0, 0.0)\n",
    "    smaup = ta.utils.sma(up,6)\n",
    "    smadn = ta.utils.sma(dn,6)\n",
    "    rmi = (smaup/(smaup+smadn))*100\n",
    "    dict_y[gal]['last_y_RMI'] = rmi\n",
    "    #stochastic oscillators\n",
    "    stochD = ta.momentum.StochasticOscillator(dict_y[gal]['last_y_max'],dict_y[gal]['last_y_min'],dict_y[gal]['last_y'], n=4, d_n=4)\n",
    "    dict_y[gal]['last_y_stochK'] = stochD.stoch()\n",
    "    dict_y[gal]['last_y_stochD'] = stochD.stoch_signal()\n",
    "    #Average True Range\n",
    "    atr = ta.volatility.AverageTrueRange(dict_y[gal]['last_y_max'],dict_y[gal]['last_y_min'],dict_y[gal]['last_y'],5)\n",
    "    dict_y[gal]['last_y_ATR'] = atr.average_true_range()\n",
    "    dict_y[gal]['last_y_CCI'] = ta.trend.cci(dict_y[gal]['last_y_max'],dict_y[gal]['last_y_min'],dict_y[gal]['last_y'], n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6xRUjqvr-K7"
   },
   "outputs": [],
   "source": [
    "y_features=['last_y','last_y_ema_4','last_likely','last_y_ema_6','last_y_ema_3','time_diff','last_y_max','last_y_min','last_likely_ema_4','last_likely_ema_6','last_y_bb_bw','last_y_bb_%b','last_y_MACD','last_y_Zscore','last_y_psy','last_y_WilliamsR','last_y_RMI','last_y_stochK','last_y_stochD','last_y_ATR','last_y_CCI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SbtfWrAr-GA"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "for i in range(len(train)):\n",
    "    gal=train.galaxy.iloc[i]\n",
    "    year=train['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "    wili=past_data[past_data.galaxy==gal]\n",
    "    az=dict_y[gal][y_features][dict_y[gal]['galactic year']==year]\n",
    "    wili=pd.concat((wili,az),axis=1)\n",
    "    alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "p-MLGW1Uowrt",
    "outputId": "194cc6d9-86be-49cc-eea0-28621f36e444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00494763 0.00792982 0.00777075 0.00857775 0.01120205]\n",
      "0.008085600480224905\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = xgb.XGBRegressor(colsample_bytree=0.3,\n",
    "                 gamma=0,       \n",
    "                 max_depth=5,\n",
    "                 min_child_weight=2.,\n",
    "                 n_estimators=222,                                                                    \n",
    "                 \n",
    "                 subsample=0.9,objective='reg:squarederror')\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.20, random_state=5)\n",
    "\n",
    "scores = -1*cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\")\n",
    "print(scores)\n",
    "print(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rd8oBI3Ipwvv"
   },
   "source": [
    "**The score improved a bit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "qmr7PZggtxol",
    "outputId": "ff0f81a2-3f2d-4400-8f95-eeb53d995392"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idh1_fut_cumax                                           0.285467\n",
       "Life_expectancy_index_fut_cumax                          0.168322\n",
       "Intergalactic Development Index (IDI), Rank_fut_cumax    0.130925\n",
       "last_y_ema_4                                             0.066951\n",
       "last_y_ema_3                                             0.051732\n",
       "last_y_ema_6                                             0.028177\n",
       "Life_expectancy_index_cumin                              0.020666\n",
       "Gross capital formation (% of GGP)_3                     0.018716\n",
       "last_y_max                                               0.018191\n",
       "Share of seats in senate (% held by female)_3            0.017855\n",
       "dtype: float32"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(colsample_bytree=0.3,\n",
    "                 gamma=0,       \n",
    "                 max_depth=5,\n",
    "                 min_child_weight=2.,\n",
    "                 n_estimators=222,                                                                    \n",
    "                 \n",
    "                 subsample=0.9,objective='reg:squarederror')\n",
    "model.fit(X_train,  y_train)\n",
    "\n",
    "feature_imp = pd.Series(model.feature_importances_,index=list(X_train.columns)).sort_values(ascending=False)\n",
    "feature_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RlDxtrF22rh"
   },
   "source": [
    "We could stop here. But since the features related to past target values are not accurate ( due to the missing values) . **We use the prediction from this first model to reconstruct the target values to more accurate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "WMeNC0Ekebgv",
    "outputId": "1909d5d0-142d-4ce7-d532-070d1a310356"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=200, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "model.fit(X_train[list(feature_imp.head(80).index)].fillna(0), np.log(y_train.fillna(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8Cp4fTOqGeN"
   },
   "source": [
    "We have trained a **Random forest using the best 80 features from Xgboost** . \n",
    "We will now evaluate on each data points and **update the y based features at every step **, so the latter datapoints benefit from the improved y features.\n",
    "We will repeat this 3 times \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyxE8HGRAXWs"
   },
   "outputs": [],
   "source": [
    "preds=[]\n",
    "for i in range(len(test)):\n",
    "    gal=test.galaxy.iloc[i]\n",
    "    year=test['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year].copy()\n",
    "    wili=past_data[past_data.galaxy==gal].copy()\n",
    "    di=dict_y[gal].copy()\n",
    "    az=di.loc[dict_y[gal]['galactic year']==year,y_features]\n",
    "    wili=pd.concat((wili,az),axis=1).copy()\n",
    "    a=wili[list(feature_imp.head(80).index)].fillna(0).copy()\n",
    "    predi=np.exp(model.predict(a)[0])\n",
    "    preds.append(predi)\n",
    "    di.loc[di['galactic year']==year,'y']=predi   \n",
    "    di.loc[di['galactic year']==year,'y']=predi\n",
    "    di['last_y']=di['y'].shift().copy()\n",
    "    di['next_y']=(di['y'].shift(-1)).fillna(method='bfill').copy()\n",
    "    di['next_y_2']=(di['next_y'].shift(-1)).copy()\n",
    "    di['next_y_3']=(di['next_y_2'].shift(-1)).copy()\n",
    "    di['last_likely']=np.log(di['y'].shift()+0.01).copy()\n",
    "\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up, down = diff.copy(), diff.copy()\n",
    "    diff[up < 0] = 0\n",
    "    diff[up > 0] = 1\n",
    "    di['last_y_psy'] = sma(diff,4).copy()\n",
    "    bb = ta.volatility.BollingerBands(di['last_y'],n=6)\n",
    "    di['last_y_bb_%b'] = bb.bollinger_pband().copy()\n",
    "    di['last_y_bb_bw'] = bb.bollinger_wband().copy()\n",
    "    \n",
    "    di['last_y_MACD'] = ta.trend.macd(di['last_y'], n_slow=5, n_fast=2).copy()\n",
    "    Zscore=(di['last_y']-sma(di['last_y'],5))/di['last_y'].rolling(5).std().copy()\n",
    "    di['last_y_Zscore']=Zscore.copy()\n",
    "    \n",
    "    \n",
    "    di['last_likely_ema_4']=ema(di['last_likely'],4,True).fillna(0).copy()\n",
    "    di['last_likely_ema_6']=ema(di['last_likely'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_4']=ema(di['last_y'],4,True).fillna(0).copy()\n",
    "    di['last_y_ema_6']=ema(di['last_y'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_3']=ema(di['last_y'],3,True).fillna(0).copy()\n",
    "    di['time_diff']=di['galactic year'].diff().copy()\n",
    "    di['last_y_max']=di['last_y'].rolling(5).max().copy()\n",
    "    di['last_y_min']=di['last_y'].rolling(5).min().copy()\n",
    "    \n",
    "    #William %R\n",
    "    wri = ta.momentum.WilliamsRIndicator(high=di['last_y_max'],low=di['last_y_min'],close=di['last_y'],lbp=6)\n",
    "    di['last_y_WilliamsR'] = wri.wr().copy()\n",
    "\n",
    "    #Relative momentum index\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up = diff.where(diff > 0, 0.0)\n",
    "    dn = -diff.where(diff < 0, 0.0)\n",
    "    smaup = ta.utils.sma(up,6)\n",
    "    smadn = ta.utils.sma(dn,6)\n",
    "    rmi = (smaup/(smaup+smadn))*100\n",
    "    di['last_y_RMI'] = rmi.copy()\n",
    "\n",
    "\n",
    "    #stochastic oscillators\n",
    "    stochD = ta.momentum.StochasticOscillator(di['last_y_max'],di['last_y_min'],di['last_y'], n=4, d_n=4)\n",
    "    di['last_y_stochK'] = stochD.stoch().copy()\n",
    "    di['last_y_stochD'] = stochD.stoch_signal().copy()\n",
    "\n",
    "    #Average True Range\n",
    "    atr = ta.volatility.AverageTrueRange(di['last_y_max'],di['last_y_min'],di['last_y'],5)\n",
    "    di['last_y_ATR'] = atr.average_true_range().copy()\n",
    "    di['last_y_CCI'] = ta.trend.cci(di['last_y_max'],di['last_y_min'],di['last_y'], n=6).copy()\n",
    "    dict_y[gal]=di.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoQ6vXDM_cJZ"
   },
   "outputs": [],
   "source": [
    "predi1=pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HL20tlHkByGQ"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "for i in range(len(train)):\n",
    "    gal=train.galaxy.iloc[i]\n",
    "    year=train['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "    wili=past_data[past_data.galaxy==gal]\n",
    "    az=dict_y[gal][y_features][dict_y[gal]['galactic year']==year]\n",
    "    wili=pd.concat((wili,az),axis=1)\n",
    "    alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03gqC3Be3r0z"
   },
   "source": [
    "# Refit the model on the improved last_y features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "YXcChBawkpcJ",
    "outputId": "ffe47f3c-9ee7-4c00-c76d-4e5ca37448b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=200, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "model.fit(X_train[list(feature_imp.head(100).index)].fillna(0), np.log(y_train.fillna(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIWLYsRu3zae"
   },
   "source": [
    "# New test Evaluation and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M93VPVqKCES0"
   },
   "outputs": [],
   "source": [
    "preds2=[]\n",
    "for i in range(len(test)):\n",
    "    gal=test.galaxy.iloc[i]\n",
    "    year=test['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year].copy()\n",
    "    wili=past_data[past_data.galaxy==gal].copy()\n",
    "    di=dict_y[gal].copy()\n",
    "    az=di.loc[dict_y[gal]['galactic year']==year,y_features]\n",
    "    wili=pd.concat((wili,az),axis=1).copy()\n",
    "    a=wili[list(feature_imp.head(100).index)].fillna(0).copy()\n",
    "    \n",
    "    predi=np.exp(model.predict(a)[0])\n",
    "    \n",
    "    preds2.append(predi)\n",
    "    \n",
    "    di.loc[di['galactic year']==year,'y']=predi\n",
    "    \n",
    "    \n",
    "    di.loc[di['galactic year']==year,'y']=predi\n",
    "    di['last_y']=di['y'].shift().copy()\n",
    "    di['next_y']=(di['y'].shift(-1)).fillna(method='bfill').copy()\n",
    "    di['next_y_2']=(di['next_y'].shift(-1)).copy()\n",
    "    di['next_y_3']=(di['next_y_2'].shift(-1)).copy()\n",
    "    di['last_likely']=np.log(di['y'].shift()+0.01).copy()\n",
    "\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up, down = diff.copy(), diff.copy()\n",
    "    diff[up < 0] = 0\n",
    "    diff[up > 0] = 1\n",
    "    di['last_y_psy'] = sma(diff,4).copy()\n",
    "    bb = ta.volatility.BollingerBands(di['last_y'],n=6)\n",
    "    di['last_y_bb_%b'] = bb.bollinger_pband().copy()\n",
    "    di['last_y_bb_bw'] = bb.bollinger_wband().copy()\n",
    "    \n",
    "    di['last_y_MACD'] = ta.trend.macd(di['last_y'], n_slow=5, n_fast=2).copy()\n",
    "    Zscore=(di['last_y']-sma(di['last_y'],5))/di['last_y'].rolling(5).std().copy()\n",
    "    di['last_y_Zscore']=Zscore.copy()\n",
    "    \n",
    "    \n",
    "    di['last_likely_ema_4']=ema(di['last_likely'],4,True).fillna(0).copy()\n",
    "    di['last_likely_ema_6']=ema(di['last_likely'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_4']=ema(di['last_y'],4,True).fillna(0).copy()\n",
    "    di['last_y_ema_6']=ema(di['last_y'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_3']=ema(di['last_y'],3,True).fillna(0).copy()\n",
    "    di['time_diff']=di['galactic year'].diff().copy()\n",
    "    di['last_y_max']=di['last_y'].rolling(5).max().copy()\n",
    "    di['last_y_min']=di['last_y'].rolling(5).min().copy()\n",
    "    \n",
    "    #William %R\n",
    "    wri = ta.momentum.WilliamsRIndicator(high=di['last_y_max'],low=di['last_y_min'],close=di['last_y'],lbp=6)\n",
    "    di['last_y_WilliamsR'] = wri.wr().copy()\n",
    "\n",
    "    #Relative momentum index\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up = diff.where(diff > 0, 0.0)\n",
    "    dn = -diff.where(diff < 0, 0.0)\n",
    "    smaup = ta.utils.sma(up,6)\n",
    "    smadn = ta.utils.sma(dn,6)\n",
    "    rmi = (smaup/(smaup+smadn))*100\n",
    "    di['last_y_RMI'] = rmi.copy()\n",
    "\n",
    "\n",
    "    #stochastic oscillators\n",
    "    stochD = ta.momentum.StochasticOscillator(di['last_y_max'],di['last_y_min'],di['last_y'], n=4, d_n=4)\n",
    "    di['last_y_stochK'] = stochD.stoch().copy()\n",
    "    di['last_y_stochD'] = stochD.stoch_signal().copy()\n",
    "\n",
    "    #Average True Range\n",
    "    atr = ta.volatility.AverageTrueRange(di['last_y_max'],di['last_y_min'],di['last_y'],5)\n",
    "    di['last_y_ATR'] = atr.average_true_range().copy()\n",
    "\n",
    "\n",
    "    di['last_y_CCI'] = ta.trend.cci(di['last_y_max'],di['last_y_min'],di['last_y'], n=6).copy()\n",
    "    dict_y[gal]=di.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2W33xA3eC5EB"
   },
   "outputs": [],
   "source": [
    "predi2=pd.DataFrame(preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F4M3lbPL5kZ"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "for i in range(len(train)):\n",
    "    gal=train.galaxy.iloc[i]\n",
    "    year=train['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "    wili=past_data[past_data.galaxy==gal]\n",
    "    az=dict_y[gal][y_features][dict_y[gal]['galactic year']==year]\n",
    "    wili=pd.concat((wili,az),axis=1)\n",
    "    alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "rrJd6Vg3oSbI",
    "outputId": "67a4c61f-4ed9-4f55-e1b3-334b93756c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=200, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "model.fit(X_train[list(feature_imp.head(80).index)].fillna(0), np.log(y_train.fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOxL7CErMEti"
   },
   "outputs": [],
   "source": [
    "preds3=[]\n",
    "for i in range(len(test)):\n",
    "    gal=test.galaxy.iloc[i]\n",
    "    year=test['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year].copy()\n",
    "    wili=past_data[past_data.galaxy==gal].copy()\n",
    "    di=dict_y[gal].copy()\n",
    "    az=di.loc[dict_y[gal]['galactic year']==year,y_features]\n",
    "    wili=pd.concat((wili,az),axis=1).copy()\n",
    "    a=wili[list(feature_imp.head(80).index)].fillna(0).copy()\n",
    "    \n",
    "    predi=np.exp(model.predict(a)[0])\n",
    "    \n",
    "    preds3.append(predi)\n",
    "    \n",
    "    di.loc[di['galactic year']==year,'y']=predi\n",
    "    \n",
    "    \n",
    "    di.loc[di['galactic year']==year,'y']=predi\n",
    "    di['last_y']=di['y'].shift().copy()\n",
    "    di['next_y']=(di['y'].shift(-1)).fillna(method='bfill').copy()\n",
    "    di['next_y_2']=(di['next_y'].shift(-1)).copy()\n",
    "    di['next_y_3']=(di['next_y_2'].shift(-1)).copy()\n",
    "    di['last_likely']=np.log(di['y'].shift()+0.01).copy()\n",
    "\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up, down = diff.copy(), diff.copy()\n",
    "    diff[up < 0] = 0\n",
    "    diff[up > 0] = 1\n",
    "    di['last_y_psy'] = sma(diff,4).copy()\n",
    "    bb = ta.volatility.BollingerBands(di['last_y'],n=6)\n",
    "    di['last_y_bb_%b'] = bb.bollinger_pband().copy()\n",
    "    di['last_y_bb_bw'] = bb.bollinger_wband().copy()\n",
    "    \n",
    "    di['last_y_MACD'] = ta.trend.macd(di['last_y'], n_slow=5, n_fast=2).copy()\n",
    "    Zscore=(di['last_y']-sma(di['last_y'],5))/di['last_y'].rolling(5).std().copy()\n",
    "    di['last_y_Zscore']=Zscore.copy()\n",
    "    \n",
    "    \n",
    "    di['last_likely_ema_4']=ema(di['last_likely'],4,True).fillna(0).copy()\n",
    "    di['last_likely_ema_6']=ema(di['last_likely'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_4']=ema(di['last_y'],4,True).fillna(0).copy()\n",
    "    di['last_y_ema_6']=ema(di['last_y'],6,True).fillna(0).copy()\n",
    "    di['last_y_ema_3']=ema(di['last_y'],3,True).fillna(0).copy()\n",
    "    di['time_diff']=di['galactic year'].diff().copy()\n",
    "    di['last_y_max']=di['last_y'].rolling(5).max().copy()\n",
    "    di['last_y_min']=di['last_y'].rolling(5).min().copy()\n",
    "    \n",
    "    #William %R\n",
    "    wri = ta.momentum.WilliamsRIndicator(high=di['last_y_max'],low=di['last_y_min'],close=di['last_y'],lbp=6)\n",
    "    di['last_y_WilliamsR'] = wri.wr().copy()\n",
    "\n",
    "    #Relative momentum index\n",
    "    diff = di['last_y'].diff(1).copy()\n",
    "    up = diff.where(diff > 0, 0.0)\n",
    "    dn = -diff.where(diff < 0, 0.0)\n",
    "    smaup = ta.utils.sma(up,6)\n",
    "    smadn = ta.utils.sma(dn,6)\n",
    "    rmi = (smaup/(smaup+smadn))*100\n",
    "    di['last_y_RMI'] = rmi.copy()\n",
    "\n",
    "\n",
    "    #stochastic oscillators\n",
    "    stochD = ta.momentum.StochasticOscillator(di['last_y_max'],di['last_y_min'],di['last_y'], n=4, d_n=4)\n",
    "    di['last_y_stochK'] = stochD.stoch().copy()\n",
    "    di['last_y_stochD'] = stochD.stoch_signal().copy()\n",
    "\n",
    "    #Average True Range\n",
    "    atr = ta.volatility.AverageTrueRange(di['last_y_max'],di['last_y_min'],di['last_y'],5)\n",
    "    di['last_y_ATR'] = atr.average_true_range().copy()\n",
    "\n",
    "\n",
    "    di['last_y_CCI'] = ta.trend.cci(di['last_y_max'],di['last_y_min'],di['last_y'], n=6).copy()\n",
    "    dict_y[gal]=di.copy()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeruQQoRqd78"
   },
   "source": [
    "We have completed the task 3 times and **believe the features are more accurate**. Of course the model will be influenced by the first bias since the features now lead to that value, it is also possible that the model slowly converges to the ideal prediction after every repetition. It did improve the lb score .\n",
    "we can now train a model on **these possibly better features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxDTWgvb9TlT"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "for i in range(len(train)):\n",
    "    gal=train.galaxy.iloc[i]\n",
    "    year=train['galactic year'].iloc[i]\n",
    "    past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "    wili=past_data[past_data.galaxy==gal]\n",
    "    az=dict_y[gal][y_features][dict_y[gal]['galactic year']==year]\n",
    "    wili=pd.concat((wili,az),axis=1)\n",
    "    alo.append(wili)\n",
    "\n",
    "X_train=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyaYGRI0oYJo"
   },
   "outputs": [],
   "source": [
    "alo=[]\n",
    "\n",
    "for i in range(len(test)):\n",
    "  gal=test.galaxy.iloc[i]\n",
    "  year=test['galactic year'].iloc[i]\n",
    "  past_data=dataset_years[dataset_years['galactic year']==year]\n",
    "  wili=past_data[past_data.galaxy==gal]\n",
    "  az=dict_y[gal][y_features][dict_y[gal]['galactic year']==year]\n",
    "  wili=pd.concat((wili,az),axis=1)\n",
    "  alo.append(wili)\n",
    "\n",
    "X_test=pd.concat(alo,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0GBgmg0qomf"
   },
   "source": [
    "We train the random forest model on the **pot_increase**, because we want an **idea of the uncertainty of this variable for the latter optimisation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "OMmUqZ989JEE",
    "outputId": "8807ea6e-678d-4033-f26e-a6e826b5f499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=200, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "model.fit( X_train[list(feature_imp.head(80).index)].fillna(0), np.log(y_train+0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGw3PhB3q0nt"
   },
   "source": [
    "# We next retrieve the prediction from every tree as a measure of uncertainty of prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhlX2rjqpVS1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results=pd.DataFrame()\n",
    "i=0\n",
    "for pred in model.estimators_:\n",
    "    results[str(i)]=pred.predict(X_test[list(feature_imp.head(80).index)].fillna(0))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQH_9DTtpnYI"
   },
   "outputs": [],
   "source": [
    "results_bis=pd.DataFrame()\n",
    "i=0\n",
    "for pred in model.estimators_:\n",
    "    results_bis[str(i)]=np.exp(results[str(i)])-0.01\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "id": "L4xNJOQTQmNV",
    "outputId": "54289786-32d3-4c75-d224-a52a7c245ace"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041550</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.041556</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.041888</td>\n",
       "      <td>0.041599</td>\n",
       "      <td>0.041978</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>0.041677</td>\n",
       "      <td>0.041753</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>0.041394</td>\n",
       "      <td>0.041710</td>\n",
       "      <td>0.042317</td>\n",
       "      <td>0.041709</td>\n",
       "      <td>0.041571</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>0.041831</td>\n",
       "      <td>0.041751</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.041848</td>\n",
       "      <td>0.041618</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.041620</td>\n",
       "      <td>0.041574</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.041635</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.041677</td>\n",
       "      <td>0.041529</td>\n",
       "      <td>0.041970</td>\n",
       "      <td>0.041849</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.041749</td>\n",
       "      <td>0.041832</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.041577</td>\n",
       "      <td>0.041408</td>\n",
       "      <td>0.041231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041633</td>\n",
       "      <td>0.041727</td>\n",
       "      <td>0.041949</td>\n",
       "      <td>0.041670</td>\n",
       "      <td>0.041889</td>\n",
       "      <td>0.041217</td>\n",
       "      <td>0.042160</td>\n",
       "      <td>0.041759</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.041619</td>\n",
       "      <td>0.041403</td>\n",
       "      <td>0.041660</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.041648</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>0.041585</td>\n",
       "      <td>0.041475</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>0.041657</td>\n",
       "      <td>0.041647</td>\n",
       "      <td>0.042015</td>\n",
       "      <td>0.041872</td>\n",
       "      <td>0.041572</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>0.041697</td>\n",
       "      <td>0.041677</td>\n",
       "      <td>0.041513</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0.041506</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>0.041207</td>\n",
       "      <td>0.041697</td>\n",
       "      <td>0.041567</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>0.041604</td>\n",
       "      <td>0.038537</td>\n",
       "      <td>0.041550</td>\n",
       "      <td>0.041569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.040188</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>0.039374</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>0.039076</td>\n",
       "      <td>0.038744</td>\n",
       "      <td>0.038954</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.038779</td>\n",
       "      <td>0.037845</td>\n",
       "      <td>0.039926</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.038732</td>\n",
       "      <td>0.038943</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038006</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.038679</td>\n",
       "      <td>0.038749</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>0.038634</td>\n",
       "      <td>0.039080</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.038892</td>\n",
       "      <td>0.037521</td>\n",
       "      <td>0.038671</td>\n",
       "      <td>0.038775</td>\n",
       "      <td>0.040144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039357</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.038584</td>\n",
       "      <td>0.038865</td>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038763</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.039015</td>\n",
       "      <td>0.038691</td>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.039421</td>\n",
       "      <td>0.039276</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>0.038601</td>\n",
       "      <td>0.039598</td>\n",
       "      <td>0.038591</td>\n",
       "      <td>0.039743</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>0.037521</td>\n",
       "      <td>0.038688</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.037978</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038178</td>\n",
       "      <td>0.037521</td>\n",
       "      <td>0.038589</td>\n",
       "      <td>0.039075</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>0.038674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038566</td>\n",
       "      <td>0.039118</td>\n",
       "      <td>0.038779</td>\n",
       "      <td>0.038589</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>0.038094</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.038035</td>\n",
       "      <td>0.038943</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038863</td>\n",
       "      <td>0.038115</td>\n",
       "      <td>0.038066</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.038634</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.038608</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.038108</td>\n",
       "      <td>0.038384</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039357</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.037542</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.037917</td>\n",
       "      <td>0.038322</td>\n",
       "      <td>0.038612</td>\n",
       "      <td>0.038756</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038763</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.029242</td>\n",
       "      <td>0.038681</td>\n",
       "      <td>0.038323</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.037856</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.038322</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>0.037521</td>\n",
       "      <td>0.038688</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>0.038661</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038178</td>\n",
       "      <td>0.038936</td>\n",
       "      <td>0.038070</td>\n",
       "      <td>0.037955</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>0.038674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038940</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.040188</td>\n",
       "      <td>0.038537</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>0.039374</td>\n",
       "      <td>0.038811</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.039076</td>\n",
       "      <td>0.038744</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>0.039118</td>\n",
       "      <td>0.038779</td>\n",
       "      <td>0.039260</td>\n",
       "      <td>0.038663</td>\n",
       "      <td>0.038666</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.038732</td>\n",
       "      <td>0.038943</td>\n",
       "      <td>0.039313</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>0.038733</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.039839</td>\n",
       "      <td>0.038749</td>\n",
       "      <td>0.039245</td>\n",
       "      <td>0.039638</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.038971</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.038954</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.038545</td>\n",
       "      <td>0.038384</td>\n",
       "      <td>0.038782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.038584</td>\n",
       "      <td>0.038868</td>\n",
       "      <td>0.038144</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.037521</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.038856</td>\n",
       "      <td>0.038763</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.039015</td>\n",
       "      <td>0.038555</td>\n",
       "      <td>0.038322</td>\n",
       "      <td>0.039435</td>\n",
       "      <td>0.039421</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>0.038601</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>0.040084</td>\n",
       "      <td>0.039101</td>\n",
       "      <td>0.038938</td>\n",
       "      <td>0.038557</td>\n",
       "      <td>0.038688</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.038517</td>\n",
       "      <td>0.038837</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.038589</td>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>0.038834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024168</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.029281</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.023422</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.024301</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.022365</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.025877</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.023601</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.021057</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.022955</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021664</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.023049</td>\n",
       "      <td>0.023095</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.024205</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.022324</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>0.023141</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.022417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.016638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.016831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.023846</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>0.022488</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.023960</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.022303</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>0.022365</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.022488</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>0.028452</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>0.022955</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.023769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.024129</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.023736</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>0.023845</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.022194</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>0.022417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0.062159</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>0.065876</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>0.062304</td>\n",
       "      <td>0.067975</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.067385</td>\n",
       "      <td>0.067181</td>\n",
       "      <td>0.076766</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.060691</td>\n",
       "      <td>0.063474</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>0.068840</td>\n",
       "      <td>0.071006</td>\n",
       "      <td>0.071102</td>\n",
       "      <td>0.066201</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>0.065731</td>\n",
       "      <td>0.066368</td>\n",
       "      <td>0.058246</td>\n",
       "      <td>0.063407</td>\n",
       "      <td>0.068530</td>\n",
       "      <td>0.065697</td>\n",
       "      <td>0.065705</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>0.065283</td>\n",
       "      <td>0.066034</td>\n",
       "      <td>0.066256</td>\n",
       "      <td>0.063392</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071092</td>\n",
       "      <td>0.058246</td>\n",
       "      <td>0.074708</td>\n",
       "      <td>0.067493</td>\n",
       "      <td>0.059724</td>\n",
       "      <td>0.055392</td>\n",
       "      <td>0.065726</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.060445</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.066368</td>\n",
       "      <td>0.076766</td>\n",
       "      <td>0.063558</td>\n",
       "      <td>0.067278</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.063474</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.068861</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>0.063641</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.059724</td>\n",
       "      <td>0.063324</td>\n",
       "      <td>0.062124</td>\n",
       "      <td>0.067385</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.065964</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.065402</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.068245</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.062211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.046320</td>\n",
       "      <td>0.054142</td>\n",
       "      <td>0.053605</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.051004</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.053222</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.049699</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>0.047743</td>\n",
       "      <td>0.053841</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.049169</td>\n",
       "      <td>0.053796</td>\n",
       "      <td>0.055251</td>\n",
       "      <td>0.047975</td>\n",
       "      <td>0.049699</td>\n",
       "      <td>0.051004</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.049707</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.026457</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.051004</td>\n",
       "      <td>0.050918</td>\n",
       "      <td>0.053351</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.026457</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.044775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.054192</td>\n",
       "      <td>0.047926</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>0.047975</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.053438</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.049973</td>\n",
       "      <td>0.051004</td>\n",
       "      <td>0.040051</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.049699</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>0.053248</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.052783</td>\n",
       "      <td>0.046320</td>\n",
       "      <td>0.050042</td>\n",
       "      <td>0.046832</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>0.049699</td>\n",
       "      <td>0.052197</td>\n",
       "      <td>0.049169</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.052034</td>\n",
       "      <td>0.047939</td>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.044790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.021611</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016614</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.019874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>890 rows  200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  ...       197       198       199\n",
       "0    0.041550  0.041496  0.041556  ...  0.038537  0.041550  0.041569\n",
       "1    0.037891  0.038086  0.040188  ...  0.039075  0.038650  0.038674\n",
       "2    0.037891  0.038086  0.037982  ...  0.037955  0.038650  0.038674\n",
       "3    0.038940  0.038700  0.040188  ...  0.038324  0.038650  0.038834\n",
       "4    0.024168  0.023954  0.024187  ...  0.022204  0.023371  0.022417\n",
       "..        ...       ...       ...  ...       ...       ...       ...\n",
       "885  0.019483  0.016352  0.015929  ...  0.016852  0.017418  0.016831\n",
       "886  0.023530  0.022246  0.023858  ...  0.022194  0.023395  0.022417\n",
       "887  0.062159  0.063517  0.065876  ...  0.068245  0.062211  0.062211\n",
       "888  0.047830  0.046320  0.054142  ...  0.047830  0.049725  0.044790\n",
       "889  0.019483  0.016352  0.016543  ...  0.016685  0.019159  0.019874\n",
       "\n",
       "[890 rows x 200 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nzyuo85wrEhP"
   },
   "source": [
    "Visualisation of the upper and lower confidence regions of the sorted mean preds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lASOoKdDQzpB"
   },
   "outputs": [],
   "source": [
    "a=pd.DataFrame()\n",
    "a['upper']=results_bis.mean(axis=1)+results_bis.std(axis=1)\n",
    "a['down']=results_bis.mean(axis=1)-results_bis.std(axis=1)\n",
    "a['moyenne']=results_bis.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "Mce2PjzJRGp5",
    "outputId": "3dc01910-5b23-48ec-f5ce-0ffeb8b74f6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5b3561bb70>,\n",
       " <matplotlib.lines.Line2D at 0x7f5b3561bcc0>,\n",
       " <matplotlib.lines.Line2D at 0x7f5b3561be48>]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc=a.sort_values('moyenne').reset_index(drop=True)\n",
    "plt.plot(cc,label=cc.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4Fa5s2YrULQ"
   },
   "source": [
    "# Construction of allocation: \n",
    "For every estimator prediction, we rank the prediction in increasing manner :\n",
    "We give the first 500 an allocation of 100.  and the rest zeros. It almost always satisfies the constraint. Of course every mistake in ranking leads to huge loss. So we hope that averaging accross the 200 tree prediction of Random Forest We will have **an allocation that accounts for the uncertainty of our predictions** . \n",
    "\n",
    "We later chose to allocate in a linear fashion between  ranks 350 and 650, as this manual manipulation seemed to account better for the uncertainty. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_TLinhhoKzP"
   },
   "outputs": [],
   "source": [
    "   \n",
    "allocation=pd.DataFrame()\n",
    "for col in results.columns:\n",
    "    bruh=pd.DataFrame(results[col].sort_values())\n",
    "    bruh['allocation']=0.\n",
    "    bruh.iloc[0:350,1]=100.\n",
    "    bruh.iloc[350:650,1]=[99.833-i*(100/300) for i in range(300)]\n",
    "    bruh=bruh.sort_index()\n",
    "    allocation[col]=bruh['allocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "id": "pfB82lag7omB",
    "outputId": "b6b56ff4-ddda-4cac-a675-d677743f9a31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.166333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.166333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.499667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.166333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.166333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.833000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.833000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>23.499667</td>\n",
       "      <td>20.499667</td>\n",
       "      <td>15.499667</td>\n",
       "      <td>33.833000</td>\n",
       "      <td>15.833000</td>\n",
       "      <td>25.166333</td>\n",
       "      <td>9.166333</td>\n",
       "      <td>31.499667</td>\n",
       "      <td>14.166333</td>\n",
       "      <td>16.166333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.166333</td>\n",
       "      <td>29.833000</td>\n",
       "      <td>16.499667</td>\n",
       "      <td>16.166333</td>\n",
       "      <td>24.833</td>\n",
       "      <td>17.499667</td>\n",
       "      <td>24.166333</td>\n",
       "      <td>33.833000</td>\n",
       "      <td>14.833000</td>\n",
       "      <td>6.833</td>\n",
       "      <td>1.833</td>\n",
       "      <td>7.166333</td>\n",
       "      <td>15.833</td>\n",
       "      <td>17.166333</td>\n",
       "      <td>18.166333</td>\n",
       "      <td>11.833000</td>\n",
       "      <td>13.499667</td>\n",
       "      <td>35.499667</td>\n",
       "      <td>19.166333</td>\n",
       "      <td>7.833000</td>\n",
       "      <td>17.499667</td>\n",
       "      <td>17.499667</td>\n",
       "      <td>26.833000</td>\n",
       "      <td>23.166333</td>\n",
       "      <td>15.166333</td>\n",
       "      <td>11.499667</td>\n",
       "      <td>15.166333</td>\n",
       "      <td>18.833000</td>\n",
       "      <td>18.833000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.499667</td>\n",
       "      <td>32.833000</td>\n",
       "      <td>3.499667</td>\n",
       "      <td>16.499667</td>\n",
       "      <td>30.499667</td>\n",
       "      <td>39.499667</td>\n",
       "      <td>17.166333</td>\n",
       "      <td>28.499667</td>\n",
       "      <td>28.833000</td>\n",
       "      <td>25.833000</td>\n",
       "      <td>17.166333</td>\n",
       "      <td>16.166333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.499667</td>\n",
       "      <td>15.166333</td>\n",
       "      <td>25.499667</td>\n",
       "      <td>21.166333</td>\n",
       "      <td>25.499667</td>\n",
       "      <td>19.499667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.833000</td>\n",
       "      <td>13.833</td>\n",
       "      <td>22.499667</td>\n",
       "      <td>16.833000</td>\n",
       "      <td>31.833</td>\n",
       "      <td>20.499667</td>\n",
       "      <td>26.833000</td>\n",
       "      <td>13.166333</td>\n",
       "      <td>30.499667</td>\n",
       "      <td>15.833</td>\n",
       "      <td>15.833000</td>\n",
       "      <td>21.166333</td>\n",
       "      <td>21.833000</td>\n",
       "      <td>12.833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.166333</td>\n",
       "      <td>10.499667</td>\n",
       "      <td>28.499667</td>\n",
       "      <td>23.166333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>63.833000</td>\n",
       "      <td>68.833000</td>\n",
       "      <td>44.166333</td>\n",
       "      <td>45.166333</td>\n",
       "      <td>66.499667</td>\n",
       "      <td>64.833000</td>\n",
       "      <td>54.166333</td>\n",
       "      <td>60.166333</td>\n",
       "      <td>48.166333</td>\n",
       "      <td>65.833000</td>\n",
       "      <td>54.499667</td>\n",
       "      <td>57.499667</td>\n",
       "      <td>73.833000</td>\n",
       "      <td>65.833000</td>\n",
       "      <td>45.833000</td>\n",
       "      <td>59.833</td>\n",
       "      <td>53.499667</td>\n",
       "      <td>67.833000</td>\n",
       "      <td>61.833000</td>\n",
       "      <td>59.499667</td>\n",
       "      <td>46.833</td>\n",
       "      <td>39.833</td>\n",
       "      <td>65.166333</td>\n",
       "      <td>55.833</td>\n",
       "      <td>55.166333</td>\n",
       "      <td>78.833000</td>\n",
       "      <td>66.166333</td>\n",
       "      <td>53.833000</td>\n",
       "      <td>44.166333</td>\n",
       "      <td>62.499667</td>\n",
       "      <td>56.166333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.833000</td>\n",
       "      <td>53.499667</td>\n",
       "      <td>55.499667</td>\n",
       "      <td>46.499667</td>\n",
       "      <td>62.833000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>59.166333</td>\n",
       "      <td>80.499667</td>\n",
       "      <td>...</td>\n",
       "      <td>62.833000</td>\n",
       "      <td>78.166333</td>\n",
       "      <td>53.499667</td>\n",
       "      <td>55.499667</td>\n",
       "      <td>44.833000</td>\n",
       "      <td>61.499667</td>\n",
       "      <td>79.166333</td>\n",
       "      <td>54.499667</td>\n",
       "      <td>64.166333</td>\n",
       "      <td>43.499667</td>\n",
       "      <td>47.833000</td>\n",
       "      <td>54.499667</td>\n",
       "      <td>53.833</td>\n",
       "      <td>53.833000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.833000</td>\n",
       "      <td>55.499667</td>\n",
       "      <td>66.833000</td>\n",
       "      <td>47.833000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>55.499667</td>\n",
       "      <td>48.833</td>\n",
       "      <td>67.833000</td>\n",
       "      <td>55.166333</td>\n",
       "      <td>68.833</td>\n",
       "      <td>44.499667</td>\n",
       "      <td>57.166333</td>\n",
       "      <td>49.833000</td>\n",
       "      <td>58.166333</td>\n",
       "      <td>62.833</td>\n",
       "      <td>97.166333</td>\n",
       "      <td>51.499667</td>\n",
       "      <td>62.499667</td>\n",
       "      <td>54.833</td>\n",
       "      <td>40.499667</td>\n",
       "      <td>46.833</td>\n",
       "      <td>60.499667</td>\n",
       "      <td>62.166333</td>\n",
       "      <td>57.499667</td>\n",
       "      <td>74.166333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>890 rows  200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2  ...         197         198         199\n",
       "0    100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "1    100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "2    100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "3    100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "4    100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "..          ...         ...         ...  ...         ...         ...         ...\n",
       "885  100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "886  100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "887   23.499667   20.499667   15.499667  ...   10.499667   28.499667   23.166333\n",
       "888   63.833000   68.833000   44.166333  ...   62.166333   57.499667   74.166333\n",
       "889  100.000000  100.000000  100.000000  ...  100.000000  100.000000  100.000000\n",
       "\n",
       "[890 rows x 200 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Bi9NVxf5kfL"
   },
   "source": [
    "Checking Constraint for mean allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oi0H1hQXsaDM",
    "outputId": "d703572a-b3f8-444e-a20c-7ab07c2c71cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6458.634651666666"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alloo=allocation.mean(axis=1)\n",
    "expec=test['existence expectancy index']\n",
    "low=pd.DataFrame()\n",
    "low['islow']=expec\n",
    "low['islow']=0.\n",
    "low['islow'][expec<0.7]=1.\n",
    "(allocation.mean(axis=1)*low.islow).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qQFmKAssW2q"
   },
   "outputs": [],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['index']=test.index\n",
    "sub['pred']=results_bis.mean(axis=1)\n",
    "sub['opt_pred']=alloo\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nIjXvLE5pNV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "data_analysis",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
